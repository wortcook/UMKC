{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(X_train, Y_train), (X_test,Y_test) = cifar10.load_data()\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model_32_3_eps.keras\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">139264</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">279,424</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │   <span style=\"color: #00af00; text-decoration-color: #00af00\">142,607,360</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,250</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ functional_91 (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m139264\u001b[0m)         │       \u001b[38;5;34m279,424\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │   \u001b[38;5;34m142,607,360\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m10,250\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">142,897,034</span> (545.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m142,897,034\u001b[0m (545.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">142,896,202</span> (545.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m142,896,202\u001b[0m (545.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> (3.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m832\u001b[0m (3.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model_32_3_eps.keras\n",
      "Epoch 1/30\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 77ms/step - accuracy: 0.3605 - loss: 277295.1250 - val_accuracy: 0.4131 - val_loss: 127232.7266\n",
      "Epoch 2/30\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 76ms/step - accuracy: 0.4451 - loss: 132765.2656 - val_accuracy: 0.4299 - val_loss: 150767.8750\n",
      "Epoch 3/30\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 77ms/step - accuracy: 0.4890 - loss: 126935.5391 - val_accuracy: 0.5407 - val_loss: 136431.8281\n",
      "Epoch 4/30\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 77ms/step - accuracy: 0.5109 - loss: 149878.9688 - val_accuracy: 0.5082 - val_loss: 170083.1875\n",
      "Epoch 5/30\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 78ms/step - accuracy: 0.5014 - loss: 193934.0312 - val_accuracy: 0.4403 - val_loss: 323586.3750\n",
      "Epoch 6/30\n",
      "\u001b[1m 909/1563\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.5098 - loss: 234428.2656"
     ]
    }
   ],
   "source": [
    "cmi = keras.Input(shape=(None,None,3))\n",
    "\n",
    "end_models = []\n",
    "\n",
    "filter_counts =  [32, 16, 16, 32, 32, 64, 64]\n",
    "kernel_sizes  =  [3,  13, 11,  9,  7,  5,  3]\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "for i in range(len(filter_counts)):\n",
    "    #convert X_sample to float32\n",
    "\n",
    "    #if the model file already exists, skip it\n",
    "    inner_model = None\n",
    "\n",
    "    x_model = None\n",
    "\n",
    "    try:\n",
    "        inner_model = models.load_model(f'model_{filter_counts[i]}_{kernel_sizes[i]}_eps.keras', compile=False)\n",
    "        print(f'Loaded model_{filter_counts[i]}_{kernel_sizes[i]}_eps.keras')\n",
    "        inner_model.trainable = True\n",
    "    except:\n",
    "\n",
    "        inp = layers.Input(shape=(32,32,3))\n",
    "\n",
    "        x1 = layers.Conv2D(filter_counts[i], (kernel_sizes[i], kernel_sizes[i]), activation='relu', padding='same')(inp)\n",
    "        x1ref = x1\n",
    "        x  = layers.Dropout(0.25)(x1)\n",
    "        x  = layers.BatchNormalization()(x)\n",
    "\n",
    "        x3 = layers.Conv2D(2*filter_counts[i], (3, 3), activation='relu', padding='same')(x)\n",
    "        x  = layers.Dropout(0.25)(x3)\n",
    "        x  = layers.BatchNormalization()(x)\n",
    "\n",
    "        x4 = layers.Conv2D(2*filter_counts[i], (3, 3), activation='relu', padding='same')(x)\n",
    "        x  = layers.Dropout(0.25)(x4)\n",
    "        x  = layers.BatchNormalization()(x)\n",
    "        x  = layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "        x5 = layers.Conv2D(4*filter_counts[i], (3, 3), activation='relu', padding='same')(x)\n",
    "        x  = layers.Dropout(0.25)(x5)\n",
    "        x  = layers.BatchNormalization()(x)\n",
    "\n",
    "        x6 = layers.Conv2D(4*filter_counts[i], (3, 3), activation='relu', padding='same')(x)\n",
    "        x  = layers.Dropout(0.25)(x6)\n",
    "        x  = layers.BatchNormalization()(x)\n",
    "        x  = layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "        xB = layers.Add()([x3, x4])\n",
    "        xC = layers.Add()([x5, x6])\n",
    "\n",
    "        xAf = layers.Flatten()(x1)\n",
    "        xBf = layers.Flatten()(xB)\n",
    "        xCf = layers.Flatten()(xC)\n",
    "        xf  = layers.Flatten()(x)\n",
    "\n",
    "        x = layers.Concatenate()([xAf, xBf, xCf, xf])\n",
    "\n",
    "        x_model = models.Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "        inner_model = models.Sequential()\n",
    "        inner_model.add(layers.InputLayer(shape=(32,32,3)))\n",
    "        inner_model.add(x_model)\n",
    "\n",
    "\n",
    "    inner_model.add(layers.Dense(1024, activation='relu'))\n",
    "    inner_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    inner_model.summary()\n",
    "\n",
    "    print(f\"Training model_{filter_counts[i]}_{kernel_sizes[i]}_eps.keras\")\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    inner_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # inner_model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "    inner_model.fit(X_train, Y_train, epochs=30, validation_data=(X_test,Y_test), verbose=1)\n",
    "\n",
    "    inner_model.pop(rebuild=True)\n",
    "    inner_model.pop(rebuild=True)\n",
    "\n",
    "    inner_model.save(f'model_{filter_counts[i]}_{kernel_sizes[i]}_eps.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasjones/.pyenv/versions/finproj/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 34 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m343/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.0687 - loss: 4.3005"
     ]
    }
   ],
   "source": [
    "#load the cifar100 dataset\n",
    "cifar100 = tf.keras.datasets.cifar100\n",
    "\n",
    "(X_train, Y_train), (X_test,Y_test) = cifar100.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "\n",
    "print(Y_test.shape)\n",
    "\n",
    "the_models = []\n",
    "\n",
    "for i in range(len(filter_counts)):\n",
    "    inner_model = models.load_model(f'model_{filter_counts[i]}_{kernel_sizes[i]}_eps.keras')\n",
    "    inner_model.trainable = False\n",
    "\n",
    "    #Fine tune the model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(shape=(32,32,3)))\n",
    "    model.add(inner_model)\n",
    "    model.add(layers.GlobalAveragePooling())\n",
    "    model.add(layers.Dense(256, activation='tanh'))\n",
    "    model.add(layers.Dense(100, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, Y_train, epochs=10, batch_size=64)\n",
    "\n",
    "    model.trainable = False\n",
    "\n",
    "    the_models.append(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_234\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_234\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_13      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_6        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,172,644</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_7        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,170,340</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_8        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,429,028</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_9        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,425,956</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_10       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,354,212</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,351,140</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ sequential_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ sequential_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ sequential_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ sequential_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ sequential_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_13      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_6        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │  \u001b[38;5;34m1,172,644\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_7        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │  \u001b[38;5;34m1,170,340\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_8        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │  \u001b[38;5;34m2,429,028\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_9        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │  \u001b[38;5;34m2,425,956\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_10       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │  \u001b[38;5;34m5,354,212\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │  \u001b[38;5;34m5,351,140\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ sequential_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ sequential_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ sequential_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ sequential_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ sequential_10[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ sequential_11[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m10,100\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,913,420</span> (68.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,913,420\u001b[0m (68.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> (39.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,100\u001b[0m (39.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,903,320</span> (68.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,903,320\u001b[0m (68.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "(50000, 100)\n",
      "(50000, 32, 32, 3)\n",
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 27ms/step - accuracy: 0.1340 - loss: 4.2435\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.3669 - loss: 3.2690\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.4278 - loss: 2.7838\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.4426 - loss: 2.5387\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.4566 - loss: 2.3641\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.4655 - loss: 2.2604\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.4751 - loss: 2.1777\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.4768 - loss: 2.1312\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.4821 - loss: 2.0763\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.4860 - loss: 2.0517\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step\n",
      "(10000, 100)\n",
      "(10000, 100)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.67      0.61       100\n",
      "           1       0.36      0.45      0.40       100\n",
      "           2       0.16      0.25      0.19       100\n",
      "           3       0.13      0.14      0.14       100\n",
      "           4       0.19      0.18      0.19       100\n",
      "           5       0.33      0.41      0.36       100\n",
      "           6       0.35      0.28      0.31       100\n",
      "           7       0.41      0.28      0.33       100\n",
      "           8       0.56      0.53      0.54       100\n",
      "           9       0.52      0.56      0.54       100\n",
      "          10       0.34      0.37      0.36       100\n",
      "          11       0.22      0.19      0.20       100\n",
      "          12       0.43      0.42      0.42       100\n",
      "          13       0.51      0.35      0.42       100\n",
      "          14       0.33      0.28      0.30       100\n",
      "          15       0.23      0.28      0.25       100\n",
      "          16       0.50      0.38      0.43       100\n",
      "          17       0.53      0.52      0.53       100\n",
      "          18       0.31      0.33      0.32       100\n",
      "          19       0.34      0.27      0.30       100\n",
      "          20       0.77      0.77      0.77       100\n",
      "          21       0.49      0.55      0.52       100\n",
      "          22       0.30      0.28      0.29       100\n",
      "          23       0.52      0.57      0.54       100\n",
      "          24       0.52      0.64      0.57       100\n",
      "          25       0.31      0.30      0.30       100\n",
      "          26       0.49      0.33      0.40       100\n",
      "          27       0.22      0.32      0.26       100\n",
      "          28       0.60      0.68      0.64       100\n",
      "          29       0.40      0.33      0.36       100\n",
      "          30       0.45      0.43      0.44       100\n",
      "          31       0.35      0.33      0.34       100\n",
      "          32       0.31      0.35      0.33       100\n",
      "          33       0.41      0.40      0.41       100\n",
      "          34       0.22      0.24      0.23       100\n",
      "          35       0.11      0.06      0.08       100\n",
      "          36       0.30      0.47      0.37       100\n",
      "          37       0.40      0.32      0.35       100\n",
      "          38       0.21      0.21      0.21       100\n",
      "          39       0.51      0.41      0.45       100\n",
      "          40       0.33      0.40      0.36       100\n",
      "          41       0.76      0.56      0.64       100\n",
      "          42       0.18      0.17      0.17       100\n",
      "          43       0.30      0.40      0.34       100\n",
      "          44       0.17      0.23      0.19       100\n",
      "          45       0.21      0.15      0.17       100\n",
      "          46       0.22      0.24      0.23       100\n",
      "          47       0.57      0.40      0.47       100\n",
      "          48       0.68      0.66      0.67       100\n",
      "          49       0.53      0.47      0.50       100\n",
      "          50       0.19      0.15      0.17       100\n",
      "          51       0.28      0.25      0.26       100\n",
      "          52       0.66      0.62      0.64       100\n",
      "          53       0.42      0.65      0.51       100\n",
      "          54       0.50      0.54      0.52       100\n",
      "          55       0.18      0.10      0.13       100\n",
      "          56       0.56      0.48      0.52       100\n",
      "          57       0.40      0.53      0.45       100\n",
      "          58       0.66      0.56      0.61       100\n",
      "          59       0.41      0.35      0.38       100\n",
      "          60       0.77      0.69      0.73       100\n",
      "          61       0.52      0.47      0.49       100\n",
      "          62       0.47      0.34      0.40       100\n",
      "          63       0.24      0.37      0.29       100\n",
      "          64       0.14      0.20      0.17       100\n",
      "          65       0.20      0.28      0.23       100\n",
      "          66       0.24      0.17      0.20       100\n",
      "          67       0.35      0.31      0.33       100\n",
      "          68       0.77      0.78      0.78       100\n",
      "          69       0.48      0.62      0.54       100\n",
      "          70       0.44      0.30      0.36       100\n",
      "          71       0.55      0.67      0.60       100\n",
      "          72       0.17      0.16      0.16       100\n",
      "          73       0.27      0.32      0.29       100\n",
      "          74       0.22      0.19      0.21       100\n",
      "          75       0.57      0.48      0.52       100\n",
      "          76       0.60      0.71      0.65       100\n",
      "          77       0.24      0.22      0.23       100\n",
      "          78       0.23      0.18      0.20       100\n",
      "          79       0.37      0.40      0.39       100\n",
      "          80       0.13      0.12      0.13       100\n",
      "          81       0.36      0.39      0.38       100\n",
      "          82       0.58      0.55      0.56       100\n",
      "          83       0.23      0.16      0.19       100\n",
      "          84       0.25      0.23      0.24       100\n",
      "          85       0.57      0.61      0.59       100\n",
      "          86       0.44      0.48      0.46       100\n",
      "          87       0.46      0.56      0.51       100\n",
      "          88       0.19      0.11      0.14       100\n",
      "          89       0.39      0.32      0.35       100\n",
      "          90       0.43      0.37      0.40       100\n",
      "          91       0.47      0.48      0.48       100\n",
      "          92       0.31      0.22      0.26       100\n",
      "          93       0.28      0.20      0.23       100\n",
      "          94       0.64      0.77      0.70       100\n",
      "          95       0.44      0.43      0.43       100\n",
      "          96       0.39      0.39      0.39       100\n",
      "          97       0.27      0.34      0.30       100\n",
      "          98       0.18      0.23      0.20       100\n",
      "          99       0.30      0.35      0.32       100\n",
      "\n",
      "    accuracy                           0.38     10000\n",
      "   macro avg       0.39      0.38      0.38     10000\n",
      "weighted avg       0.39      0.38      0.38     10000\n",
      "\n",
      "[[67  2  0 ...  0  0  0]\n",
      " [ 2 45  1 ...  0  0  2]\n",
      " [ 3  1 25 ...  0  9  0]\n",
      " ...\n",
      " [ 0  0  1 ... 34  0  1]\n",
      " [ 1  1  8 ...  0 23  1]\n",
      " [ 0  1  2 ...  0  0 35]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#make the models trainable\n",
    "for model in the_models:\n",
    "    model.trainable = False\n",
    "\n",
    "\n",
    "#Construct an ensemble model\n",
    "inp = layers.Input(shape=(32,32,3))\n",
    "x = layers.Add()([model(inp) for model in the_models])\n",
    "x = layers.Dense(100, activation='softmax')(x)\n",
    "\n",
    "ensemble_model = models.Model(inp, x)\n",
    "\n",
    "print(ensemble_model.summary())\n",
    "print(Y_train.shape)\n",
    "print(X_train.shape)\n",
    "\n",
    "ensemble_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "ensemble_model.fit(X_train, Y_train, epochs=10, batch_size=64)\n",
    "\n",
    "#Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "print(Y_pred.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "yp_max = np.argmax(Y_pred, axis=1)\n",
    "yt_max = np.argmax(Y_test, axis=1)\n",
    "\n",
    "#print the metrics\n",
    "print(classification_report(yt_max, yp_max))\n",
    "\n",
    "print(confusion_matrix(yt_max, yp_max))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67,  2,  0, ...,  0,  0,  0],\n",
       "       [ 2, 45,  1, ...,  0,  0,  2],\n",
       "       [ 3,  1, 25, ...,  0,  9,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  1, ..., 34,  0,  1],\n",
       "       [ 1,  1,  8, ...,  0, 23,  1],\n",
       "       [ 0,  1,  2, ...,  0,  0, 35]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(yt_max, yp_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
